{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exhuastive Framework for Risk Management and Anomaly Detection in Highly Imbalanced Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As technology continues to advance, the prevalence of cyber security risks has become more pronounced. The digital landscape provides ample opportunities for malicious actors to exploit vulnerabilities in various systems by ways of phishing attacks, ransomware, social engineering tactics, and detection evading automated programs and scripts. Machine learning provides an invaluable role in risk management and can fortify threat detection mechanisms to identify and prevent fradulent activities, and maintaining the integrity of systems and transactions.\n",
    "\n",
    "However, finding the right balance between robust anomaly detection and user convenience is a chllenging task for organizations, and machine learning models tend to perform well on the majority class, leading to potential oversight of minority class instances such as rare anomaly cases. In this tutorial, we will explore the predictive strengths of several machine learning techniques such as Random Forest, AdaBoost, CatBoost, XGBoost, and LightGBM, on both the majority and minority class, and compare techniques to address the class imbalance issue using oversampling (E.g., SMOTE, ADASYN) and stratified sampling (E.g., Stratified K-Fold Cross Validation) techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be found in Kaggle, and contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly imbalanced, with the positive class (frauds) accounting for 0.172% of all transactions.\n",
    "\n",
    "The dataset contains only numerical inputs:\n",
    "- __Time__ contains the seconds elapsed between each transaction and the first transaction in the dataset\n",
    "- __Amount__ containing the transaction Amount\n",
    "- __Class__ is the response variable and it takes value 1 in case of fraud and 0 otherwise\n",
    "- __V1, V2, ... , V28__ are the principal components of the PCA transformation of the other original features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly import tools\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import gc\n",
    "from datetime import datetime \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn import svm\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import xgboost as xgb\n",
    "import os\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.callbacks import DeadlineStopper, DeltaYStopper\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "%matplotlib inline \n",
    "init_notebook_mode(connected=True)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.168375e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.379537e-15</td>\n",
       "      <td>2.074095e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.487313e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.213481e-16</td>\n",
       "      <td>-2.406331e-15</td>\n",
       "      <td>2.239053e-15</td>\n",
       "      <td>1.673327e-15</td>\n",
       "      <td>-1.247012e-15</td>\n",
       "      <td>8.190001e-16</td>\n",
       "      <td>1.207294e-15</td>\n",
       "      <td>4.887456e-15</td>\n",
       "      <td>1.437716e-15</td>\n",
       "      <td>-3.772171e-16</td>\n",
       "      <td>9.564149e-16</td>\n",
       "      <td>1.039917e-15</td>\n",
       "      <td>6.406204e-16</td>\n",
       "      <td>1.654067e-16</td>\n",
       "      <td>-3.568593e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.473266e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.683437e-15</td>\n",
       "      <td>-3.660091e-16</td>\n",
       "      <td>-1.227390e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>1.088850e+00</td>\n",
       "      <td>1.020713e+00</td>\n",
       "      <td>9.992014e-01</td>\n",
       "      <td>9.952742e-01</td>\n",
       "      <td>9.585956e-01</td>\n",
       "      <td>9.153160e-01</td>\n",
       "      <td>8.762529e-01</td>\n",
       "      <td>8.493371e-01</td>\n",
       "      <td>8.381762e-01</td>\n",
       "      <td>8.140405e-01</td>\n",
       "      <td>7.709250e-01</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>-2.458826e+01</td>\n",
       "      <td>-4.797473e+00</td>\n",
       "      <td>-1.868371e+01</td>\n",
       "      <td>-5.791881e+00</td>\n",
       "      <td>-1.921433e+01</td>\n",
       "      <td>-4.498945e+00</td>\n",
       "      <td>-1.412985e+01</td>\n",
       "      <td>-2.516280e+01</td>\n",
       "      <td>-9.498746e+00</td>\n",
       "      <td>-7.213527e+00</td>\n",
       "      <td>-5.449772e+01</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>-5.354257e-01</td>\n",
       "      <td>-7.624942e-01</td>\n",
       "      <td>-4.055715e-01</td>\n",
       "      <td>-6.485393e-01</td>\n",
       "      <td>-4.255740e-01</td>\n",
       "      <td>-5.828843e-01</td>\n",
       "      <td>-4.680368e-01</td>\n",
       "      <td>-4.837483e-01</td>\n",
       "      <td>-4.988498e-01</td>\n",
       "      <td>-4.562989e-01</td>\n",
       "      <td>-2.117214e-01</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>-9.291738e-02</td>\n",
       "      <td>-3.275735e-02</td>\n",
       "      <td>1.400326e-01</td>\n",
       "      <td>-1.356806e-02</td>\n",
       "      <td>5.060132e-02</td>\n",
       "      <td>4.807155e-02</td>\n",
       "      <td>6.641332e-02</td>\n",
       "      <td>-6.567575e-02</td>\n",
       "      <td>-3.636312e-03</td>\n",
       "      <td>3.734823e-03</td>\n",
       "      <td>-6.248109e-02</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>4.539234e-01</td>\n",
       "      <td>7.395934e-01</td>\n",
       "      <td>6.182380e-01</td>\n",
       "      <td>6.625050e-01</td>\n",
       "      <td>4.931498e-01</td>\n",
       "      <td>6.488208e-01</td>\n",
       "      <td>5.232963e-01</td>\n",
       "      <td>3.996750e-01</td>\n",
       "      <td>5.008067e-01</td>\n",
       "      <td>4.589494e-01</td>\n",
       "      <td>1.330408e-01</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>2.374514e+01</td>\n",
       "      <td>1.201891e+01</td>\n",
       "      <td>7.848392e+00</td>\n",
       "      <td>7.126883e+00</td>\n",
       "      <td>1.052677e+01</td>\n",
       "      <td>8.877742e+00</td>\n",
       "      <td>1.731511e+01</td>\n",
       "      <td>9.253526e+00</td>\n",
       "      <td>5.041069e+00</td>\n",
       "      <td>5.591971e+00</td>\n",
       "      <td>3.942090e+01</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.487313e-15 -5.556467e-16  1.213481e-16 -2.406331e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "                V10           V11           V12           V13           V14  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   2.239053e-15  1.673327e-15 -1.247012e-15  8.190001e-16  1.207294e-15   \n",
       "std    1.088850e+00  1.020713e+00  9.992014e-01  9.952742e-01  9.585956e-01   \n",
       "min   -2.458826e+01 -4.797473e+00 -1.868371e+01 -5.791881e+00 -1.921433e+01   \n",
       "25%   -5.354257e-01 -7.624942e-01 -4.055715e-01 -6.485393e-01 -4.255740e-01   \n",
       "50%   -9.291738e-02 -3.275735e-02  1.400326e-01 -1.356806e-02  5.060132e-02   \n",
       "75%    4.539234e-01  7.395934e-01  6.182380e-01  6.625050e-01  4.931498e-01   \n",
       "max    2.374514e+01  1.201891e+01  7.848392e+00  7.126883e+00  1.052677e+01   \n",
       "\n",
       "                V15           V16           V17           V18           V19  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   4.887456e-15  1.437716e-15 -3.772171e-16  9.564149e-16  1.039917e-15   \n",
       "std    9.153160e-01  8.762529e-01  8.493371e-01  8.381762e-01  8.140405e-01   \n",
       "min   -4.498945e+00 -1.412985e+01 -2.516280e+01 -9.498746e+00 -7.213527e+00   \n",
       "25%   -5.828843e-01 -4.680368e-01 -4.837483e-01 -4.988498e-01 -4.562989e-01   \n",
       "50%    4.807155e-02  6.641332e-02 -6.567575e-02 -3.636312e-03  3.734823e-03   \n",
       "75%    6.488208e-01  5.232963e-01  3.996750e-01  5.008067e-01  4.589494e-01   \n",
       "max    8.877742e+00  1.731511e+01  9.253526e+00  5.041069e+00  5.591971e+00   \n",
       "\n",
       "                V20           V21           V22           V23           V24  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   6.406204e-16  1.654067e-16 -3.568593e-16  2.578648e-16  4.473266e-15   \n",
       "std    7.709250e-01  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min   -5.449772e+01 -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%   -2.117214e-01 -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%   -6.248109e-02 -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    1.330408e-01  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    3.942090e+01  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('C://Users//wlim129//Desktop//Work//Anomaly Detection//creditcard.csv')\n",
    "\n",
    "print(data_df.shape)\n",
    "# data has rows: 284807  columns: 31\n",
    "\n",
    "data_df.head()\n",
    "\n",
    "data_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing data\n",
    "\n",
    "total = data_df.isnull().sum().sort_values(ascending = False)\n",
    "percent = (data_df.isnull().sum()/data_df.isnull().count()*100).sort_values(ascending = False)\n",
    "pd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset does not contain any missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for data imbalance\n",
    "\n",
    "temp = data_df[\"Class\"].value_counts()\n",
    "df = pd.DataFrame({'Class': temp.index,'values': temp.values})\n",
    "\n",
    "trace = go.Bar(\n",
    "    x = df['Class'],y = df['values'],\n",
    "    name=\"Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)\",\n",
    "    marker=dict(color=\"Red\"),\n",
    "    text=df['values']\n",
    ")\n",
    "data = [trace]\n",
    "layout = dict(title = 'Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)',\n",
    "          xaxis = dict(title = 'Class', showticklabels=True), \n",
    "          yaxis = dict(title = 'Number of transactions'),\n",
    "          hovermode = 'closest',width=600\n",
    "         )\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig, filename='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 492 (or 0.172%) of transactions are fraudulent. The data is highly imbalanced with respect to the target variable Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time density plot for transactions\n",
    "\n",
    "class_0 = data_df.loc[data_df['Class'] == 0][\"Time\"]\n",
    "class_1 = data_df.loc[data_df['Class'] == 1][\"Time\"]\n",
    "\n",
    "hist_data = [class_0, class_1]\n",
    "group_labels = ['Not Fraud', 'Fraud']\n",
    "\n",
    "fig = ff.create_distplot(hist_data, group_labels, show_hist=False, show_rug=False)\n",
    "fig['layout'].update(title='Credit Card Transactions Time Density Plot', xaxis=dict(title='Time [s]'))\n",
    "iplot(fig, filename='dist_only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fraudulent transactions are more evenly distributed compared to valid transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical summaries of fraud and non-fraud transactions by hour\n",
    "\n",
    "data_df['Hour'] = data_df['Time'].apply(lambda x: np.floor(x / 3600))\n",
    "\n",
    "tmp = data_df.groupby(['Hour', 'Class'])['Amount'].aggregate(['min', 'max', 'count', 'sum', 'mean', 'median', 'var']).reset_index()\n",
    "df = pd.DataFrame(tmp)\n",
    "df.columns = ['Hour', 'Class', 'Min', 'Max', 'Transactions', 'Sum', 'Mean', 'Median', 'Var']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total amount by class, non-fraud is blue and fraud is red\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Sum\", data=df.loc[df.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Sum\", data=df.loc[df.Class==1], color=\"red\")\n",
    "plt.suptitle(\"Total Amount\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of transactions by class, non-fraud is blue and fraud is red\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Transactions\", data=df.loc[df.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Transactions\", data=df.loc[df.Class==1], color=\"red\")\n",
    "plt.suptitle(\"Total Number of Transactions\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average amount per transaction by class, non-fraud is blue and fraud is red\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Mean\", data=df.loc[df.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Mean\", data=df.loc[df.Class==1], color=\"red\")\n",
    "plt.suptitle(\"Average Amount per Transaction\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum amount of transaction by class, non-fraud is blue and fraud is red\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Mean\", data=df.loc[df.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Mean\", data=df.loc[df.Class==1], color=\"red\")\n",
    "plt.suptitle(\"Maximum Amount of Transactions\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median amount of transaction by class, non-fraud is blue and fraud is red\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Median\", data=df.loc[df.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Median\", data=df.loc[df.Class==1], color=\"red\")\n",
    "plt.suptitle(\"Median Amount of Transactions\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum amount of transaction by class, non-fraud is blue and fraud is red\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))\n",
    "s = sns.lineplot(ax = ax1, x=\"Hour\", y=\"Min\", data=df.loc[df.Class==0])\n",
    "s = sns.lineplot(ax = ax2, x=\"Hour\", y=\"Min\", data=df.loc[df.Class==1], color=\"red\")\n",
    "plt.suptitle(\"Minimum Amount of Transactions\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics of both classes, with and without outliers\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\n",
    "s = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=True)\n",
    "s = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=data_df, palette=\"PRGn\",showfliers=False)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data_df[['Amount','Class']].copy()\n",
    "class_0 = tmp.loc[tmp['Class'] == 0]['Amount']\n",
    "class_1 = tmp.loc[tmp['Class'] == 1]['Amount']\n",
    "class_0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using median as the average, real transactions have a larger average value and Q1, smaller Q3 and larger outliers. Fraudulent transactions have a smaller average and Q1, larger Q3 and smaller outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of fradulent transactions, amount against time in seconds\n",
    "\n",
    "fraud = data_df.loc[data_df['Class'] == 1]\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x = fraud['Time'],y = fraud['Amount'],\n",
    "    name=\"Amount\",\n",
    "     marker=dict(\n",
    "                color='rgb(238,23,11)',\n",
    "                line=dict(\n",
    "                    color='red',\n",
    "                    width=1),\n",
    "                opacity=0.5,\n",
    "            ),\n",
    "    text= fraud['Amount'],\n",
    "    mode = \"markers\"\n",
    ")\n",
    "data = [trace]\n",
    "layout = dict(title = 'Amount of fraudulent transactions',\n",
    "          xaxis = dict(title = 'Time [s]', showticklabels=True), \n",
    "          yaxis = dict(title = 'Amount'),\n",
    "          hovermode='closest'\n",
    "         )\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig, filename='fraud-amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features correlation\n",
    "\n",
    "plt.figure(figsize = (14,14))\n",
    "plt.title('Credit Card Transactions features correlation plot (Pearson)')\n",
    "corr = data_df.corr()\n",
    "sns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because V1-V28 is obtained from PCA, we notice that there is no notable correlation between them. However, there are certain correlations between some of these features and Time (inverse correlation with V3) and Amount (direct correlation with V7 and V20, inverse correlation with V1 and V5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression line for Amount against V20 and V7\n",
    "\n",
    "s = sns.lmplot(x='V20', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\n",
    "s = sns.lmplot(x='V7', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the two couples of features are correlated (the regression lines for Class = 0 have a positive slope, whilst the regression line for Class = 1 have a smaller positive slope)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression line for Amount against V2 and V5\n",
    "\n",
    "s = sns.lmplot(x='V2', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\n",
    "s = sns.lmplot(x='V5', y='Amount',data=data_df, hue='Class', fit_reg=True,scatter_kws={'s':2})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the two couples of features are inverse correlated (the regression lines for Class = 0 have a negative slope while the regression lines for Class = 1 have a very small negative slope)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features density plot\n",
    "\n",
    "var = data_df.columns.values\n",
    "\n",
    "i = 0\n",
    "t0 = data_df.loc[data_df['Class'] == 0]\n",
    "t1 = data_df.loc[data_df['Class'] == 1]\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(8,4,figsize=(16,28))\n",
    "\n",
    "for feature in var:\n",
    "    i += 1\n",
    "    plt.subplot(8,4,i)\n",
    "    sns.kdeplot(t0[feature], bw_method=0.5,label=\"Class = 0\", warn_singular=False)\n",
    "    sns.kdeplot(t1[feature], bw_method=0.5,label=\"Class = 1\", warn_singular=False)\n",
    "    plt.xlabel(feature, fontsize=12)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V4 and V11 have clearly separated distributions for Class values 0 and 1. V12, V14, V18 are partially separated. V1, V2, V3, V10 have a quite distinct profile, whilst V25, V26, V28 have similar profiles for the two values of Class.\n",
    "\n",
    "In general, with just few exceptions (Time and Amount), the features distribution for legitimate transactions is centered around 0, sometime with a long queue at one of the extremities, while the fraudulent transactions (values of Class = 1) have a skewed (asymmetric) distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified Train/Validation/Test split\n",
    "\n",
    "target = 'Class'\n",
    "\n",
    "VALID_SIZE = 0.20 # simple validation using train_test_split\n",
    "TEST_SIZE = 0.20 # test size using_train_test_split\n",
    "RANDOM_STATE = 333\n",
    "\n",
    "X = data_df.drop([target, 'Hour'],axis = 1)\n",
    "Y = data_df[target]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, stratify=Y, random_state=RANDOM_STATE, shuffle=True)\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=VALID_SIZE, stratify=Y_train, random_state=RANDOM_STATE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now proceed with scaling. Max-min Normalization features will have a small Standard deviation compared to Standardization. Normalization will scale most of the data to a small interval, which means all features will have a small scale but do not handle outliers well. Whereas, Standardization is robust to outliers. They transform the probability distribution for an input variable to standard Gaussian Standardization and can become skewed or biased if the input variable contains outlier values.\n",
    "\n",
    "To overcome this, the median and Interquartile range can be used when standardizing numerical input variables which technique is referred to as robust scaling. Robust scaling uses percentile to scale numerical input variables that contain outliers by scaling numerical input variables using the median and interquartile range. It calculated the median, 25th, and 75th percentiles. The value of each variable is then subtracted with median and divided by Interquartile range (IQR). Value = (value- median) / (p75 â€” p25)\n",
    "\n",
    "This results in variable having mean to 0, median and standard deviation to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robust scaling\n",
    "\n",
    "rob_scaler_amount = RobustScaler()\n",
    "rob_scaler_time = RobustScaler()\n",
    "rob_scaler_amount.fit(X_train['Amount'].values.reshape(-1,1))\n",
    "rob_scaler_time.fit(X_train['Time'].values.reshape(-1,1))\n",
    "X_train['Time'] = rob_scaler_amount.transform(X_train['Amount'].values.reshape(-1,1))\n",
    "X_train['Amount'] = rob_scaler_time.transform(X_train['Time'].values.reshape(-1,1))\n",
    "X_valid['Time'] = rob_scaler_amount.transform(X_valid['Amount'].values.reshape(-1,1))\n",
    "X_valid['Amount'] = rob_scaler_time.transform(X_valid['Time'].values.reshape(-1,1))\n",
    "X_test['Time'] = rob_scaler_amount.transform(X_test['Amount'].values.reshape(-1,1))\n",
    "X_test['Amount'] = rob_scaler_time.transform(X_test['Time'].values.reshape(-1,1))\n",
    "X_train.head()\n",
    "X_valid.head()\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our train data is highly imbalance with way fewer fraudulent case (less than 0.2%) compared to non-fraudulent case, we can risk achieving high accuracy by consistently predicting the majority class but not the minority class. To tackle this, we can apply oversampling technique, specifically choosing ADASYN instead of other techniques like SMOTE to balance the highly imbalance training data. The main difference between them is that ADASYN generates synthetic samples adaptively based on the density distribution of minority class samples, while SMOTE generates synthetic samples by interpolating between minority class samples. For cases where data is highly imbalance, ADASYN is often considered more suitable as it focuses on generating synthetic samples in regions of the feature space where the class imbalance is most pronounced, but SMOTE treats all minority class instances equally in a uniform approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling train data with ADASYN\n",
    "\n",
    "X_train, Y_train = ADASYN(random_state=RANDOM_STATE).fit_resample(X_train, Y_train)\n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the RFC model\n",
    "\n",
    "RFC_METRIC = 'gini'\n",
    "NUM_ESTIMATORS = 100\n",
    "NO_JOBS = -1\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=NO_JOBS, \n",
    "                             random_state=RANDOM_STATE,\n",
    "                             criterion=RFC_METRIC,\n",
    "                             n_estimators=NUM_ESTIMATORS,\n",
    "                             verbose=False)\n",
    "\n",
    "clf.fit(X_train, Y_train.values)\n",
    "\n",
    "preds = clf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "\n",
    "tmp = pd.DataFrame({'Feature': X_train.columns.to_numpy(), 'Feature importance': clf.feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=14)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important features are V4, V14, V12, V17, V18, V10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "cm = pd.crosstab(Y_valid.values, preds, rownames=['Actual'], colnames=['Predicted'])\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n",
    "sns.heatmap(cm, \n",
    "            xticklabels=['Not Fraud', 'Fraud'],\n",
    "            yticklabels=['Not Fraud', 'Fraud'],\n",
    "            annot=True,ax=ax1,\n",
    "            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC\n",
    "\n",
    "roc_auc_score(Y_valid.values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC-AUC score obtained with RandomForestClassifier is 0.87."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the AdaBoost model\n",
    "\n",
    "clf = AdaBoostClassifier(random_state=RANDOM_STATE,\n",
    "                         algorithm='SAMME.R',\n",
    "                         learning_rate=0.8,\n",
    "                         n_estimators=NUM_ESTIMATORS)\n",
    "\n",
    "clf.fit(X_train, Y_train.values)\n",
    "\n",
    "preds = clf.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "\n",
    "tmp = pd.DataFrame({'Feature': X_train.columns.to_numpy(), 'Feature importance': clf.feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=14)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "cm = pd.crosstab(Y_valid.values, preds, rownames=['Actual'], colnames=['Predicted'])\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n",
    "sns.heatmap(cm, \n",
    "            xticklabels=['Not Fraud', 'Fraud'],\n",
    "            yticklabels=['Not Fraud', 'Fraud'],\n",
    "            annot=True,ax=ax1,\n",
    "            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC\n",
    "\n",
    "roc_auc_score(Y_valid.values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC-AUC score obtained with AdaBoostClassifier is 0.91."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoostClassifier\n",
    "\n",
    "VERBOSE_EVAL = 50\n",
    "\n",
    "clf = CatBoostClassifier(iterations=500,\n",
    "                         learning_rate=0.02,\n",
    "                         depth=12,\n",
    "                         eval_metric='AUC',\n",
    "                         random_seed = RANDOM_STATE,\n",
    "                         bagging_temperature = 0.2,\n",
    "                         od_type='Iter',\n",
    "                         metric_period = VERBOSE_EVAL,\n",
    "                         od_wait=100)\n",
    "\n",
    "clf.fit(X_train, Y_train.values, eval_set=[(X_train, Y_train), (X_valid, Y_valid)])\n",
    "\n",
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "\n",
    "tmp = pd.DataFrame({'Feature': X_train.columns.to_numpy(), 'Feature importance': clf.feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=14)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = pd.crosstab(Y_test.values, preds, rownames=['Actual'], colnames=['Predicted'])\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n",
    "sns.heatmap(cm, \n",
    "            xticklabels=['Not Fraud', 'Fraud'],\n",
    "            yticklabels=['Not Fraud', 'Fraud'],\n",
    "            annot=True,ax=ax1,\n",
    "            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC\n",
    "\n",
    "roc_auc_score(Y_test.values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC-AUC score obtained with CatBoostClassifier is 0.92."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the train and valid datasets\n",
    "dtrain = xgb.DMatrix(X_train, Y_train.values)\n",
    "dvalid = xgb.DMatrix(X_valid, Y_valid.values)\n",
    "dtest = xgb.DMatrix(X_test, Y_test.values)\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "# set parameters\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eta'] = 0.01\n",
    "params['silent'] = True\n",
    "params['max_depth'] = 2\n",
    "params['subsample'] = 0.8\n",
    "params['colsample_bytree'] = 0.9\n",
    "params['eval_metric'] = 'auc'\n",
    "params['random_state'] = RANDOM_STATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "MAX_ROUNDS = 1000\n",
    "EARLY_STOP = 50\n",
    "\n",
    "model = xgb.train(params, \n",
    "                  dtrain, \n",
    "                  MAX_ROUNDS, \n",
    "                  watchlist, \n",
    "                  early_stopping_rounds=EARLY_STOP, \n",
    "                  maximize=True, \n",
    "                  verbose_eval=VERBOSE_EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "\n",
    "fig, (ax) = plt.subplots(ncols=1, figsize=(8,5))\n",
    "xgb.plot_importance(model, height=0.8, title=\"Features importance (XGBoost)\", ax=ax, color=\"green\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "preds = model.predict(dtest)\n",
    "preds = [round(value) for value in preds]\n",
    "\n",
    "cm = pd.crosstab(Y_test.values, preds, rownames=['Actual'], colnames=['Predicted'])\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n",
    "sns.heatmap(cm, \n",
    "            xticklabels=['Not Fraud', 'Fraud'],\n",
    "            yticklabels=['Not Fraud', 'Fraud'],\n",
    "            annot=True,ax=ax1,\n",
    "            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC\n",
    "\n",
    "roc_auc_score(Y_test.values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score for the prediction of fresh data (test set) for XGBoost is 0.92."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "lgbm = LGBMClassifier(boosting_type= 'gbdt',\n",
    "                      objective= 'binary',\n",
    "                      metric='auc',\n",
    "                      learning_rate= 0.01,\n",
    "                      num_leaves= 7,  \n",
    "                      max_depth= 4,  \n",
    "                      min_child_samples= 100, \n",
    "                      max_bin= 100,  \n",
    "                      subsample= 0.9, \n",
    "                      subsample_freq= 1, \n",
    "                      colsample_bytree= 0.7,  \n",
    "                      min_child_weight= 0,  \n",
    "                      min_split_gain= 0,  \n",
    "                      n_jobs=os.cpu_count(),\n",
    "                      verbose= 1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "\n",
    "lgbm.fit(X_train, Y_train.values, eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n",
    "        eval_metric='auc')\n",
    "\n",
    "preds = lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "\n",
    "tmp = pd.DataFrame({'Feature': X_train.columns.to_numpy(), 'Feature importance': lgbm.feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=14)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "cm = pd.crosstab(Y_test.values, preds, rownames=['Actual'], colnames=['Predicted'])\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n",
    "sns.heatmap(cm, \n",
    "            xticklabels=['Not Fraud', 'Fraud'],\n",
    "            yticklabels=['Not Fraud', 'Fraud'],\n",
    "            annot=True,ax=ax1,\n",
    "            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC\n",
    "\n",
    "roc_auc_score(Y_test.values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC-ROC score obtained with LightGBM is 0.93."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed that amongst all the model, LightGBM produces the best results with the highest accuracy.\n",
    "\n",
    "To further improve our model, we will proceed with hyperparameter tuning with Bayesian optimization and use stratified K-Fold cross validation technique for our training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian optimization, stratified K-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial parameters\n",
    "\n",
    "X = data_df.drop([target, 'Hour'],axis = 1)\n",
    "Y = data_df[target]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=TEST_SIZE, stratify=Y, random_state=RANDOM_STATE, shuffle=True)\n",
    "\n",
    "lgbm = LGBMClassifier(boosting_type='dart',\n",
    "                      objective='binary',\n",
    "                      metric='auc',\n",
    "                      n_jobs=os.cpu_count(), \n",
    "                      verbose=-1,\n",
    "                      random_state=RANDOM_STATE,\n",
    "                      min_split_gain= 0, # or can regularize next step by searching L1/L2 \n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying search space\n",
    "\n",
    "search_spaces = {\n",
    "    'learning_rate': Real(0.001, 0.01, 'log-uniform'),   \n",
    "    'n_estimators' : Integer(50, 200),\n",
    "    'num_leaves': Integer(10, 100),                    \n",
    "    'max_depth': Integer(15, 100),                       \n",
    "    'subsample': Real(0.1, 1.0, 'uniform'),           \n",
    "    'subsample_freq': Integer(0, 10),                   \n",
    "    'min_child_samples': Integer(10, 200),  \n",
    "    #'reg_lambda': Real(1e-9, 100.0, 'log-uniform'),      # L2 regularization\n",
    "    #'reg_alpha': Real(1e-9, 100.0, 'log-uniform'),       # L1 regularization\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the optimizer\n",
    "\n",
    "opt = BayesSearchCV(estimator=lgbm,                                    \n",
    "                    search_spaces=search_spaces,                                              \n",
    "                    cv=5, # stratified K-Fold automatically used here (refer to documentation)\n",
    "                    n_iter=30,                                   \n",
    "                    n_points=3,                                       \n",
    "                    n_jobs=-1,                                       \n",
    "                    return_train_score=False,                         \n",
    "                    refit=False,                                      \n",
    "                    optimizer_kwargs={'base_estimator': 'GP'},       \n",
    "                    random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the optimizer\n",
    "\n",
    "np.int = np.int64 # skopt uses np.int which was deprecated, so we change it to np.int64 manually\n",
    "opt.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "\n",
    "best_param = opt.best_params_\n",
    "\n",
    "# OrderedDict([('learning_rate', 0.008334804024808152),\n",
    "#             ('max_depth', 24),\n",
    "#             ('min_child_samples', 18),\n",
    "#             ('n_estimators', 188),\n",
    "#             ('num_leaves', 58),\n",
    "#             ('subsample', 0.9953948691683815),\n",
    "#             ('subsample_freq', 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.best_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score have an AUC-ROC of 0.9994118808839343."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the test set\n",
    "\n",
    "lgbm = LGBMClassifier(boosting_type='dart',\n",
    "                      objective='binary',\n",
    "                      metric='auc',\n",
    "                      n_jobs=os.cpu_count(), \n",
    "                      verbose=-1,\n",
    "                      random_state=RANDOM_STATE,\n",
    "                      min_split_gain= 0,\n",
    "                      learning_rate=best_param['learning_rate'],\n",
    "                      max_depth=best_param['max_depth'],\n",
    "                      min_child_samples=best_param['min_child_samples'],\n",
    "                      n_estimators=best_param['n_estimators'],\n",
    "                      num_leaves=best_param['num_leaves'],\n",
    "                      subsample=best_param['subsample'],\n",
    "                      subsample_freq=best_param['subsample_freq'],\n",
    "                      )\n",
    "\n",
    "lgbm.fit(X_train, Y_train.values)\n",
    "\n",
    "preds = lgbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "\n",
    "tmp = pd.DataFrame({'Feature': X_train.columns.to_numpy(), 'Feature importance': lgbm.feature_importances_})\n",
    "tmp = tmp.sort_values(by='Feature importance',ascending=False)\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title('Features importance',fontsize=14)\n",
    "s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n",
    "s.set_xticklabels(s.get_xticklabels(),rotation=90)\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "cm = pd.crosstab(Y_test.values, preds, rownames=['Actual'], colnames=['Predicted'])\n",
    "fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n",
    "sns.heatmap(cm, \n",
    "            xticklabels=['Not Fraud', 'Fraud'],\n",
    "            yticklabels=['Not Fraud', 'Fraud'],\n",
    "            annot=True,ax=ax1,\n",
    "            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC\n",
    "\n",
    "roc_auc_score(Y_test.values, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC-ROC score obtained with hyperparameter tuned LightGBM is 0.85."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observe, even with hyperparameter tuning, the model performed worse compared to our previous model without tuning. This shows that the highly imbalance nature of our dataset can skew results drastically, and stratified K-Fold without ovesampling techqniues is not enough to address the imbalance nature. It is better to oversample our minority cases on the training data prior to training the model.\n",
    "\n",
    "The improvement to the model would be to create an algorithm to oversample the training data for each stratified K-Folds individually, before training the model and validating them with their respective validation sets. As this method is computationally intensive, I will leave it up to readers to attempt this and compare the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
